---
services:
  systems-manager-mcp:
    image: docker.io/knucklessg1/systems-manager:latest
    # build: . # Debug
    container_name: systems-manager-mcp
    hostname: systems-manager-mcp
    command: [ "systems-manager-mcp" ]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    restart: always
    env_file:
      - .env

    environment:
      - "PYTHONUNBUFFERED=1"
      - "HOST=0.0.0.0"
      - "PORT=8010"
      - "TRANSPORT=streamable-http"
    ports:
      - "8010:8010"
    healthcheck:
      test: [ "CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8010/health')" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  systems-manager-agent:
    image: docker.io/knucklessg1/systems-manager:latest
    # build: . # Debug
    container_name: systems-manager-agent
    hostname: systems-manager-agent
    command: [ "systems-manager-agent" ]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - systems-manager-mcp
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    restart: always
    env_file:
      - .env

    environment:
      - "PYTHONUNBUFFERED=1"
      - "HOST=0.0.0.0"
      - "PORT=9009"
      - "MCP_URL=http://systems-manager-mcp:8010/mcp"
      - "PROVIDER=openai"
      - "LLM_BASE_URL=${LLM_BASE_URL:-http://host.docker.internal:1234/v1}"
      - "LLM_API_KEY=${LLM_API_KEY:-llama}"
      - "MODEL_ID=${MODEL_ID:-qwen/qwen3-coder-next}"
      - "DEBUG=False"
      - "ENABLE_WEB_UI=True"
    ports:
      - "9009:9009"
    healthcheck:
      test: [ "CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:9009/health')" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
